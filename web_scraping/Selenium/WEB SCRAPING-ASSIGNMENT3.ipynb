{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da1eb13c622284f",
   "metadata": {},
   "source": [
    "## Web scraping assignment 3\n",
    "* ### Name: Rajib Dutta\n",
    "* ### Email: duttarajib78@gmail.com\n",
    "* ### Internship Batch: DS2402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28296dc960cea76e",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \n",
    "\"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Keys, ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "driver=webdriver.Edge()\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, \"//input[@id='twotabsearchtextbox']\"))).send_keys('guitar')\n",
    "WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, \"//input[@id='nav-search-submit-button']\"))).click()\n",
    "\n",
    "page_count=1\n",
    "brands=[]\n",
    "names=[]\n",
    "prices=[]\n",
    "deliveries=[]\n",
    "availabilities=[]\n",
    "urls=[]\n",
    "product_df=pd.DataFrame(columns=['brand','name','price','delivery','availability','url'])\n",
    "while 1:\n",
    "    if page_count>3:\n",
    "        break\n",
    "    parent_window=driver.current_window_handle\n",
    "    items=WebDriverWait(driver, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//a[@class=\"a-link-normal s-no-outline\"]')))\n",
    "    for item in items:\n",
    "        try:\n",
    "            url=item.get_attribute('href')\n",
    "            urls.append(url)\n",
    "            driver.execute_script(\"arguments[0].click()\", item)\n",
    "        except Exception as e:\n",
    "            print('Item detail not found')\n",
    "            continue\n",
    "        child_window=driver.window_handles[-1]\n",
    "        driver.switch_to.window(child_window)\n",
    "        try:\n",
    "            name_xpath='//div[@id=\"titleSection\"]/h1/span'\n",
    "            name=WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, name_xpath))).text\n",
    "            names.append(name)\n",
    "        except Exception as e:\n",
    "            print('Title not found')\n",
    "            names.append('-')\n",
    "        try:\n",
    "            brand_xpath='//table[@class=\"a-normal a-spacing-micro\"]/tbody/tr[1]/td[2]/span'\n",
    "            brand=WebDriverWait(driver, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, brand_xpath)))[0].text\n",
    "            brands.append(brand)\n",
    "        except Exception as e:\n",
    "            print('Brand not found')\n",
    "            brands.append('-')\n",
    "        try:\n",
    "            price_xpath='//span[@class=\"a-price aok-align-center reinventPricePriceToPayMargin priceToPay\"]/span[2]/span[2]'\n",
    "            price=WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, price_xpath))).text\n",
    "            prices.append(price)\n",
    "        except Exception as e:\n",
    "            print('Price not found')\n",
    "            prices.append('-')\n",
    "        try:\n",
    "            delivery_xpath='//div[@id=\"deliveryBlockMessage\"]/div/div[1]/span/span'\n",
    "            delivery=WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, delivery_xpath))).text\n",
    "            deliveries.append(delivery)\n",
    "        except Exception as e:\n",
    "            print('Delivery not found')\n",
    "            deliveries.append('-')\n",
    "        try:\n",
    "            availability_xpath='//div[@id=\"availability\"]/span'\n",
    "            availability=WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, availability_xpath))).text\n",
    "            availabilities.append(availability)\n",
    "        except Exception as e:\n",
    "            print('Availability not found')\n",
    "            availabilities.append('-')\n",
    "        driver.close()\n",
    "        driver.switch_to.window(parent_window)\n",
    "    try:\n",
    "        next_button_xpath='//span[@class=\"s-pagination-strip\"]/a'\n",
    "        next_button=WebDriverWait(driver, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, next_button_xpath)))[-1]\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        print('Found via primary XPATH...Next page visited')\n",
    "    except Exception as e:\n",
    "        print('Next button primary XPATH not found, trying the secondary XPATH...')\n",
    "        next_button_xpath='//span[@class=\"s-pagination-strip\"]/ul/li/span/a'\n",
    "        try:\n",
    "            next_button=WebDriverWait(driver, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, next_button_xpath)))[-1]\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            print('Found via secondary XPATH...Next page visited')\n",
    "        except Exception as e:\n",
    "            print('Next button not enabled')\n",
    "            break\n",
    "    page_count+=1\n",
    "driver.quit()\n",
    "product_df.brand=brands\n",
    "product_df.name=names\n",
    "product_df.price=prices\n",
    "product_df.delivery=deliveries\n",
    "product_df.availability=availabilities\n",
    "product_df.url=urls\n",
    "with open(file='./guitars.csv', mode='w', encoding='utf-8') as f:\n",
    "    product_df.to_csv(path_or_buf=f,index=False)\n",
    "    f.close()"
   ],
   "id": "bfb79d09833d0e97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.",
   "id": "d6682e0383b20b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from  selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def search_images(url, search_bar_xpath, search_button_xpath, category):\n",
    "    browser=webdriver.Edge()\n",
    "    browser.maximize_window()\n",
    "    actions=ActionChains(browser)\n",
    "    browser.get(url)\n",
    "    search_bar=WebDriverWait(browser, 10).until(expected_conditions.element_to_be_clickable((By.XPATH, search_bar_xpath)))\n",
    "    search_button=WebDriverWait(browser, 10).until(expected_conditions.element_to_be_clickable((By.XPATH, search_button_xpath)))\n",
    "    actions.click(on_element=search_bar)\n",
    "    actions.send_keys(category)\n",
    "    actions.click(on_element=search_button)\n",
    "    actions.perform()\n",
    "    actions.reset_actions()\n",
    "    return browser\n",
    "\n",
    "search_bar_xpath='/html[1]/body[1]/div[1]/div[3]/form[1]/div[1]/div[1]/div[1]/div[1]/div[2]/textarea[1]'\n",
    "search_button_xpath=\"/html[1]/body[1]/div[1]/div[3]/form[1]/div[1]/div[1]/div[1]/button[1]/div[1]/span[1]/*[name()='svg'][1]\"\n",
    "categories=['fruits','cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "category_index=0\n",
    "while True:\n",
    "    if category_index==len(categories):\n",
    "        break\n",
    "    category=categories[category_index]\n",
    "    browser=search_images('https://images.google.com/', search_bar_xpath, search_button_xpath, category)    \n",
    "    images=None\n",
    "    try:\n",
    "        images=WebDriverWait(browser, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//img[@class=\"YQ4gaf\"]')))\n",
    "    except Exception as e:\n",
    "        print('Images not found...retrying')\n",
    "        browser.quit()\n",
    "        continue\n",
    "    file_path=(f'./pictures/{category}')\n",
    "    os.makedirs(file_path, exist_ok=True)\n",
    "    for i, image in enumerate(images):\n",
    "        if i==10:\n",
    "            break\n",
    "        file_name=f'{file_path}/{category}_{i}.png'\n",
    "        with open(file=file_name, mode='wb') as f:\n",
    "            try:\n",
    "                f.write(image.screenshot_as_png)\n",
    "            except Exception as e:\n",
    "                print('Failed to save image...retrying')\n",
    "                browser.quit()\n",
    "                continue\n",
    "            f.close()\n",
    "    browser.quit()\n",
    "    category_index+=1"
   ],
   "id": "a8f28fc08ecbc78d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped:\n",
    "* “Brand Name”, \n",
    "* “Smartphone name”, \n",
    "* “Colour”, \n",
    "* “RAM”, \n",
    "* “Storage(ROM)”, \n",
    "* “Primary Camera”, \n",
    "* “Secondary Camera”, \n",
    "* “Display Size”, \n",
    "* “Battery Capacity”, \n",
    "* “Price”, \n",
    "* “Product URL”. \n",
    "#### Incase if any of the details is missing then replace it by “-“. Save your results in a dataframe and CSV."
   ],
   "id": "d7df5f37e71953c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from selenium.common import NoSuchElementException\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "def search_phones_on_flipcart(phone_keyword):\n",
    "    browser=webdriver.Edge()\n",
    "    browser.maximize_window()\n",
    "    browser.get('''https://www.flipkart.com/''')\n",
    "    search_bar_xpath='/html[1]/body[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/header[1]/div[1]/div[2]/form[1]/div[1]/div[1]/input[1]'\n",
    "    search_button_xpath=\"/html[1]/body[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/header[1]/div[1]/div[2]/form[1]/div[1]/button[1]/*[name()='svg'][1]\"\n",
    "    WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, search_bar_xpath))).send_keys(phone_keyword)\n",
    "    WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, search_button_xpath))).click()\n",
    "    return browser\n",
    "\n",
    "browser=None\n",
    "phones=None\n",
    "while True:\n",
    "    if phones!=None:\n",
    "        break\n",
    "    browser=search_phones_on_flipcart('Oneplus Nord')\n",
    "    try:\n",
    "        phones=WebDriverWait(browser, 10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//a[@class=\"CGtC98\"]')))\n",
    "    except Exception as e:\n",
    "        print('Phones not found...retrying')\n",
    "        browser.quit()\n",
    "        continue\n",
    "    \n",
    "parent_window=browser.current_window_handle\n",
    "phone_df=pd.DataFrame(columns=['Brand Name', 'Smartphone name', 'Colour', 'RAM', 'Storage(ROM)', 'Primary Camera', 'Secondary Camera', 'Display Size', 'Battery Capacity', 'Price', 'Product URL'])\n",
    "for i, phone in enumerate(phones):\n",
    "    url=phone.get_attribute('href')\n",
    "    browser.execute_script(\"arguments[0].click()\", phone)\n",
    "    \n",
    "    browser.switch_to.window(browser.window_handles[-1])\n",
    "    read_more_xpath='//button[@class=\"QqFHMw _4FgsLt\"]'\n",
    "    read_more_button=browser.find_element(By.XPATH, read_more_xpath)\n",
    "    browser.execute_script(\"arguments[0].click()\", read_more_button)\n",
    "    \n",
    "    try:\n",
    "        brand=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//span[@class=\"VU-ZEz\"]'))).text\n",
    "        brand=re.compile(pattern='^\\w+').findall(string=brand)[0]\n",
    "    except Exception as e:\n",
    "        print('Brand not found',e)\n",
    "        brand='-'\n",
    "    try:\n",
    "        phone_name=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[1]/table/tbody/tr[3]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print('Phone name not found', e)\n",
    "        phone_name='-'\n",
    "    try:\n",
    "        color=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[1]/table/tbody/tr[4]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print('Color not found', e)\n",
    "        color='-'\n",
    "    try:\n",
    "        ram=WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[2]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print('RAM not found', e)\n",
    "        ram='-'\n",
    "    try:\n",
    "        storage=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[1]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print('Storage not found', e)\n",
    "        storage='-'\n",
    "    try:\n",
    "        primary_camera=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[1]/td[2]/ul/li'))).text\n",
    "        if len(re.compile(pattern='^\\d+').findall(string=primary_camera))==0:\n",
    "            primary_camera=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[2]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print(f'{i}: Primary Camera not found', e)\n",
    "        primary_camera='-'\n",
    "    try:\n",
    "        secondary_camera=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[2]/td[2]/ul/li'))).text\n",
    "        if len(re.compile(pattern='^\\d+').findall(string=secondary_camera))==0:\n",
    "            secondary_camera='-'\n",
    "        elif len(re.compile(pattern='.*Rear.*').findall(string=secondary_camera))>0:\n",
    "            secondary_camera='-'\n",
    "    except Exception as e:\n",
    "        print(f'{i}: Secondary Camera not found', e)\n",
    "        secondary_camera='-'\n",
    "    try:\n",
    "        display_size=WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[2]/table/tbody/tr[1]/td[2]/ul/li'))).text\n",
    "    except Exception as e:\n",
    "        print('Display size not found', e)\n",
    "        display_size='-'\n",
    "    try:\n",
    "        battery_capacity=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[7]/table/tbody/tr[1]/td[2]/ul/li'))).text\n",
    "        if len(re.compile(pattern='.*mAh.*').findall(battery_capacity))==0:\n",
    "            raise NoSuchElementException('Battery capacity not found..retrying')\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            battery_capacity=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_1UhVsV\"]/div[9]/table/tbody/tr[1]/td[2]/ul/li'))).text\n",
    "            if len(re.compile(pattern='.*mAh.*').findall(battery_capacity))==0:\n",
    "                raise NoSuchElementException('Battery capacity not found..retrying')\n",
    "        except Exception as e:\n",
    "            print('Retry failed..No battery capacity found', e)\n",
    "            battery_capacity='-'\n",
    "    try:\n",
    "        price=WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, '//div[@class=\"_30jeq3 _16Jk6d\"]'))).text.replace('₹', '').replace(',','')\n",
    "    except Exception as e:\n",
    "        print('Price not found')\n",
    "        price='-'\n",
    "    phone_df.loc[i]=(brand, phone_name, color, ram, storage, primary_camera, secondary_camera, display_size, battery_capacity, price, url)\n",
    "    browser.close()\n",
    "    browser.switch_to.window(parent_window)\n",
    "browser.quit()\n",
    "phone_df"
   ],
   "id": "df29f824466e00ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.",
   "id": "5c5ff8cebf5be353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "browser.get('https://www.google.com/maps')\n",
    "city=input('Enter city name: ')\n",
    "WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH, '//input[@id=\"searchboxinput\"]'))).send_keys(city)\n",
    "WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, '//span[@class=\"google-symbols\"]'))).click()\n",
    "ActionChains(browser).pause(5).perform()\n",
    "try:\n",
    "    result=WebDriverWait(browser,10).until(expected_conditions.presence_of_element_located((By.XPATH,'//h1[@class=\"DUwDvf lfPIob\"]'))).text.strip()\n",
    "    city_url=browser.current_url\n",
    "    lat, long=city_url.split(sep='@', maxsplit=2)[-1].split(sep=',',maxsplit=3)[:-1][0], city_url.split(sep='@', maxsplit=2)[-1].split(sep=',',maxsplit=3)[:-1][-1]\n",
    "    print(f'Latitude and Longitude of {result} : {lat} and {long}')\n",
    "except Exception as e:\n",
    "    print('City not found')\n",
    "finally:\n",
    "    browser.quit()"
   ],
   "id": "716ff3e1731460c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Write a program to scrap all the available details of best gaming laptops from digit.in.",
   "id": "46f24a2e5ffeeb78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from collections import ChainMap\n",
    "\n",
    "browser=webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "browser.get('https://www.digit.in/')\n",
    "\n",
    "WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, \"//input[@id='woocommerce-product-search-field-0']\"))).send_keys('Gaming Laptop')\n",
    "WebDriverWait(browser, 10).until(expected_conditions.presence_of_element_located((By.XPATH, \"//div[@class='search head_search position-relative']//i[@class='rhicon rhi-search']\"))).click()\n",
    "parent_window=browser.current_window_handle\n",
    "\n",
    "laptop_details=[]\n",
    "while True:\n",
    "    i=0\n",
    "    while True:\n",
    "        laptops=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//a[@class=\"img-centered-flex rh-flex-justify-center rh-flex-center-align\"]')))\n",
    "        if i==len(laptops):\n",
    "            break\n",
    "        laptop_link=(WebDriverWait(browser,10)\n",
    "                     .until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//a[@class=\"img-centered-flex rh-flex-justify-center rh-flex-center-align\"]')))[i]\n",
    "                     .get_attribute('href'))\n",
    "        browser.execute_script(\"window.open()\")\n",
    "        browser.switch_to.window(browser.window_handles[-1])\n",
    "        browser.get(laptop_link)\n",
    "        try:\n",
    "            keys=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//th[@class=\"woocommerce-product-attributes-item__label\"]')))\n",
    "            values=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, '//td[@class=\"woocommerce-product-attributes-item__value\"]/p')))\n",
    "        except Exception as e:\n",
    "            print('Laptop details not found', e)\n",
    "            browser.close()\n",
    "            browser.switch_to.window(parent_window)\n",
    "            i+=1\n",
    "            continue\n",
    "        laptop_dict={}\n",
    "        for key,value in zip(keys,values):\n",
    "            laptop_dict[key.text]=value.text\n",
    "        laptop_details.append(laptop_dict)\n",
    "        browser.close()\n",
    "        browser.switch_to.window(parent_window)\n",
    "        i+=1\n",
    "    try:\n",
    "        browser.refresh()\n",
    "        next_button=WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, '//a[@class=\"next page-numbers\"]')))\n",
    "        next_button.click()\n",
    "        print('Next page visited')\n",
    "    except Exception as e:\n",
    "        print('All available pages have been explored',e)\n",
    "        break\n",
    "browser.quit()\n",
    "for k in ChainMap(*laptop_details):\n",
    "    for laptop_dict in laptop_details:\n",
    "        laptop_dict.setdefault(k,'-')\n",
    "laptop_df=pd.DataFrame(laptop_details)\n",
    "laptop_df"
   ],
   "id": "6fc359b397e65226",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.",
   "id": "c4bf0fcf67c6f7c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "browser=webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "browser.get('https://www.forbes.com/billionaires/')\n",
    "\n",
    "first_row_XPATH='/html[1]/body[1]/div[1]/div[2]/div[1]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]/div[2]/div[1]'\n",
    "rank_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[1]/div'\n",
    "name_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[2]'\n",
    "net_worth_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[3]'\n",
    "age_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[4]'\n",
    "citizenship_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[5]'\n",
    "source_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[6]'\n",
    "industry_XPATH='//div[@class=\"Table_tableRow__lF_cY\"]/div[7]'\n",
    "\n",
    "billionaires_df=pd.DataFrame(columns=['rank','name','net_worth','age','citizenship','source','industry'])\n",
    "page_index=1\n",
    "df_index=0\n",
    "while True:\n",
    "    first_row=WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, first_row_XPATH)))\n",
    "    ActionChains(browser).move_to_element(to_element=first_row).click(on_element=first_row).perform()\n",
    "    i=0\n",
    "    while True:\n",
    "        if i==len(WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, rank_XPATH)))):\n",
    "            break\n",
    "            \n",
    "        rank=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, rank_XPATH)))[i].text.strip()\n",
    "        rank=int(re.compile(pattern='\\d+').search(string=rank).group(0))\n",
    "        name=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, name_XPATH)))[i].text.strip()\n",
    "        new_worth=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, net_worth_XPATH)))[i].text.strip()\n",
    "        age=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, age_XPATH)))[i].text.strip()\n",
    "        citizenship=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, citizenship_XPATH)))[i].text.strip()\n",
    "        source=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, source_XPATH)))[i].text.strip()\n",
    "        industry=WebDriverWait(browser,10).until(expected_conditions.presence_of_all_elements_located((By.XPATH, industry_XPATH)))[i].text.strip()\n",
    "        billionaires_df.loc[df_index]=(rank, name, new_worth, age, citizenship, source, industry)\n",
    "        i+=1\n",
    "        df_index+=1\n",
    "    try:\n",
    "        next_button=WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, '//div[@class=\"qXW-dtvf\"]/button[7]')))\n",
    "        ActionChains(browser).move_to_element(to_element=next_button).click(on_element=next_button).perform()\n",
    "        page_index+=1\n",
    "        print(f'Page {page_index} visited')\n",
    "    except Exception as e:\n",
    "        print('All available pages have been explored',e)\n",
    "        break\n",
    "browser.quit()\n",
    "billionaires_df"
   ],
   "id": "4147a647dfdb49c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description.",
   "id": "18973989bb953996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser=webdriver.Chrome()\n",
    "browser.maximize_window()\n",
    "browser.get('https://www.hostelworld.com/')\n",
    "\n",
    "location_search_box_XPATH=\"/html[1]/body[1]/div[3]/div[1]/div[2]/main[1]/header[1]/div[1]/div[2]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[2]/input[1]\"\n",
    "city_XPATH=\"//div[@class='label'][normalize-space()='London']\"\n",
    "lets_go_XPATH=\"//button[@class='large-button btn-content']//span[@class='text']\"\n",
    "\n",
    "location_search_box=WebDriverWait(browser,10).until(expected_conditions.visibility_of_element_located((By.XPATH, location_search_box_XPATH)))\n",
    "ActionChains(browser).click(location_search_box).send_keys('London').perform()\n",
    "location=WebDriverWait(browser,10).until(expected_conditions.visibility_of_element_located((By.XPATH, city_XPATH)))\n",
    "lets_go_button=WebDriverWait(browser,10).until(expected_conditions.visibility_of_element_located((By.XPATH, lets_go_XPATH)))\n",
    "ActionChains(browser).move_to_element(to_element=location).click(on_element=location).click(on_element=lets_go_button).pause(5).perform()\n",
    "\n",
    "parent_window=browser.current_window_handle\n",
    "\n",
    "properties_XPATH='//div[@class=\"property-list\"]/div/div/div[5]/div'\n",
    "\n",
    "properties=WebDriverWait(browser,10).until(expected_conditions.visibility_of_all_elements_located((By.XPATH, properties_XPATH)))\n",
    "hostel_df=pd.DataFrame(columns=['name','rating','distance','total_reviews','overall_review','privates_from','dorms_from','description'])\n",
    "for i,_ in enumerate(properties):\n",
    "    nameWE=WebDriverWait(browser,10).until(expected_conditions.visibility_of_all_elements_located((By.XPATH, properties_XPATH)))[i]\n",
    "    name=nameWE.find_element(By.XPATH, './/div[@class=\"property-name\"]/span').text.strip()\n",
    "    rating=nameWE.find_element(By.XPATH, './/span[@class=\"number\"]').text.strip()\n",
    "    distance=nameWE.find_element(By.XPATH, './/span[@class=\"distance-description\"]').text.strip()\n",
    "    total_reviews=nameWE.find_element(By.XPATH, './/span[@class=\"left-margin\"]').text.strip()\n",
    "    total_reviews=re.compile(pattern='\\d+').search(string=total_reviews).group(0)\n",
    "    overall_review=nameWE.find_element(By.XPATH, './/span[@class=\"keyword\"]').text.strip()\n",
    "    prices=nameWE.find_elements(By.XPATH, './/strong[@class=\"current\"]')\n",
    "    if len(prices)==2:\n",
    "        privates_from=prices[0].text.strip()\n",
    "        dorms_from=prices[-1].text.strip()\n",
    "    else:\n",
    "        privates_from='-'\n",
    "        dorms_from=prices[-1].text.strip()\n",
    "    hostel_detail_page_anchor=nameWE.find_element(By.XPATH,'.//a[@class=\"property-card-container horizontal\"]')\n",
    "    ActionChains(browser).key_down(Keys.CONTROL).click(on_element=hostel_detail_page_anchor).key_up(Keys.CONTROL).perform()\n",
    "    browser.switch_to.window(browser.window_handles[-1])\n",
    "    try:\n",
    "        show_more_XPATH='/html[1]/body[1]/div[3]/div[1]/div[1]/div[2]/section[1]/div[10]/div[1]/div[2]/div[6]/div[1]/a[1]'\n",
    "        description_XPATH='/html[1]/body[1]/div[3]/div[1]/div[1]/div[2]/section[1]/div[10]/div[1]/div[2]/div[6]/div[1]/div[2]'\n",
    "        try:\n",
    "            WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, show_more_XPATH))).click()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            description=WebDriverWait(browser,10).until(expected_conditions.visibility_of_element_located((By.XPATH, description_XPATH))).text.strip()\n",
    "        except Exception as e:\n",
    "            description='-'\n",
    "    except Exception as e:\n",
    "        show_more_XPATH='/html[1]/body[1]/div[3]/div[1]/div[1]/div[2]/section[1]/div[10]/div[1]/div[2]/div[5]/div[1]/a[1]'\n",
    "        description_XPATH='/html[1]/body[1]/div[3]/div[1]/div[1]/div[2]/section[1]/div[10]/div[1]/div[2]/div[5]/div[1]/div[2]'\n",
    "        try:\n",
    "            WebDriverWait(browser,10).until(expected_conditions.element_to_be_clickable((By.XPATH, show_more_XPATH))).click()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            description=WebDriverWait(browser,10).until(expected_conditions.visibility_of_element_located((By.XPATH, description_XPATH))).text.strip()\n",
    "        except Exception as e:\n",
    "            description='-'\n",
    "    browser.close()\n",
    "    browser.switch_to.window(parent_window)\n",
    "    hostel_df.loc[i]=(name,rating,distance,total_reviews,overall_review,privates_from,dorms_from,description)\n",
    "browser.quit()\n",
    "hostel_df"
   ],
   "id": "9eff586d565faff8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
